{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del texto, extraemos todo el texto del pdf y lo guardamos en un archivo de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracción completada. El texto se guardó en '../data/cooked\\texto_extraido.txt'.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"  # Extrae texto de cada página\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Ruta de tu PDF\n",
    "pdf_path = \"../data/raw/leyes_ajedrez.pdf\"  # Cambia esto por la ubicación de tu archivo\n",
    "\n",
    "# Extraer texto\n",
    "texto_extraido = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Crear carpeta si no existe    \n",
    "output_dir = \"../data/cooked\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el texto en un archivo dentro de 'data/cooked'\n",
    "output_path = os.path.join(output_dir, \"texto_extraido.txt\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(texto_extraido)\n",
    "\n",
    "print(f\"✅ Extracción completada. El texto se guardó en '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La base de datos elegida, ha sido ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almaz\\.conda\\envs\\prueba_rag_tailor\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\almaz\\.conda\\envs\\prueba_rag_tailor\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\almaz\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de la primera línea: [-4.75044623e-02  4.89124469e-02  1.27341738e-02  3.71110216e-02\n",
      " -6.25089034e-02  6.24664910e-02  1.16436899e-01  1.00796316e-02\n",
      "  4.63477075e-02  3.58318053e-02  2.38445606e-02  1.86951656e-03\n",
      " -4.92336135e-03  1.57713201e-02  3.41440667e-03 -7.20255524e-02\n",
      " -2.60634460e-02  6.74121603e-02 -6.22904599e-02  6.52326504e-03\n",
      "  6.79132789e-02 -2.33350545e-02 -4.56841663e-03  1.08755054e-02\n",
      " -9.58510786e-02 -3.89817655e-02  5.08660404e-03  1.80085637e-02\n",
      " -3.48556116e-02 -7.94901252e-02  8.54951441e-02 -6.16623648e-03\n",
      "  3.67506035e-02 -4.58656140e-02 -6.46752566e-02 -4.30934355e-02\n",
      "  5.96923865e-02 -3.52040194e-02  5.32939211e-02  5.61981611e-02\n",
      " -1.11851528e-01  6.75243279e-03 -9.78955925e-02 -4.36949432e-02\n",
      " -2.68482175e-02 -5.19135632e-02  5.89504763e-02 -4.86038029e-02\n",
      "  1.68348681e-02  5.84590202e-03 -3.57686505e-02 -6.13384880e-02\n",
      "  2.84125417e-04  3.52543741e-02  4.63330820e-02  8.03012308e-03\n",
      "  3.54612730e-02  2.48460993e-02  3.17950733e-02  1.13081478e-03\n",
      "  7.17853233e-02  5.04170842e-02 -1.16556026e-01  3.65999192e-02\n",
      " -6.35519475e-02 -9.78633836e-02  6.45354316e-02  1.71494707e-02\n",
      " -5.41512929e-02 -4.21260856e-02  4.94452268e-02 -6.59079105e-02\n",
      "  2.67171580e-03  6.23515099e-02 -3.87025066e-02  7.04011321e-02\n",
      " -3.15172076e-02 -2.13392396e-02  1.23620462e-02 -1.14138372e-01\n",
      "  1.27168810e-02 -8.50746930e-02 -6.88427985e-02  6.86556380e-03\n",
      " -1.08076520e-02  3.03841801e-03  3.45751818e-04  1.24612842e-02\n",
      "  1.23949960e-01  2.84584928e-02  1.90608520e-02  5.48701137e-02\n",
      " -9.04197767e-02 -2.25137938e-02  1.26951272e-02  9.02222097e-03\n",
      "  5.36161140e-02 -1.73600193e-03 -4.48196717e-02  1.41072497e-01\n",
      "  8.45330581e-02  7.69907832e-02  8.76096264e-02  4.99406457e-02\n",
      " -1.80542562e-02 -8.40341393e-03 -4.26705740e-03 -9.86802205e-03\n",
      " -1.98432151e-02 -8.45510699e-03 -3.76608372e-02 -4.75593954e-02\n",
      "  2.79836878e-02  4.40640040e-02 -8.06178432e-03 -1.00647397e-02\n",
      "  9.79274139e-02 -4.60561402e-02  2.40202639e-02 -8.18423182e-02\n",
      "  3.02797128e-02 -5.06592973e-04 -1.04215532e-03 -1.27741490e-02\n",
      "  3.44632007e-02  8.41966271e-03  2.43636519e-02  1.04492655e-34\n",
      " -5.90379648e-02 -5.43019734e-02 -5.07263318e-02  6.26770481e-02\n",
      " -2.61942279e-02  5.16749769e-02 -2.71590576e-02 -1.42998593e-02\n",
      " -2.56727589e-03 -2.41812766e-02 -8.02464224e-03 -5.47834039e-02\n",
      "  1.67864598e-02 -6.45117089e-02 -1.67489622e-03  6.93068728e-02\n",
      " -5.04355542e-02 -1.55576486e-02 -5.05594388e-02 -8.93459190e-03\n",
      " -8.62737447e-02  8.24768618e-02  2.17418633e-02  1.82894412e-02\n",
      " -1.67918280e-02 -3.01058963e-02  2.47153565e-02 -4.26301956e-02\n",
      " -3.55868079e-02  3.02434638e-02  8.23343322e-02  5.41530401e-02\n",
      " -8.20654817e-03  1.16411792e-02  4.53024246e-02  1.43058263e-02\n",
      "  3.51780802e-02 -6.04889281e-02  1.09419655e-02 -4.19548526e-02\n",
      "  8.49995911e-02 -5.08884061e-03  3.24804708e-02  8.60204101e-02\n",
      " -3.48674543e-02  4.12665680e-02 -8.20005015e-02  3.80382873e-02\n",
      "  4.65678573e-02  5.79270087e-02  2.63898894e-02 -3.69012691e-02\n",
      " -6.44682795e-02 -4.09634598e-02 -3.27477008e-02  7.70236477e-02\n",
      " -1.33114904e-01  9.05724149e-03  6.21876642e-02 -4.79019666e-03\n",
      "  2.19306704e-02  4.70192134e-02  1.82545092e-02  1.15371216e-02\n",
      " -7.80667760e-04 -1.06495835e-01 -2.42232322e-03 -6.01920597e-02\n",
      "  4.26694937e-02 -6.39910810e-03 -5.31026088e-02 -2.48081312e-02\n",
      " -1.91008560e-02  6.67137802e-02  2.36085127e-03 -1.87982712e-02\n",
      " -2.24481616e-02  3.30207013e-02 -2.27936376e-02 -7.85423722e-03\n",
      " -1.09523855e-01  2.90179942e-02  8.85900185e-02  5.16201667e-02\n",
      "  1.53374135e-01  5.90051301e-02 -3.01746242e-02 -6.06812537e-03\n",
      " -9.86765884e-03  6.16618656e-02  2.65663285e-02  6.90471008e-02\n",
      " -2.35266313e-02 -1.06831402e-01  6.11752272e-02 -1.72377890e-33\n",
      " -2.71595269e-02  5.12711518e-03  1.40519952e-02  4.29340675e-02\n",
      " -3.20915855e-03 -1.18167782e-02 -5.00749797e-02  6.01130687e-02\n",
      " -1.09027876e-02 -8.02550986e-02  1.41143482e-02 -8.87261629e-02\n",
      "  1.29110530e-01 -2.77687935e-03  4.90845218e-02 -5.04523404e-02\n",
      "  5.38769029e-02 -1.03869587e-01 -3.03189158e-02  7.62815103e-02\n",
      " -2.73440406e-02  4.23726737e-02  1.08678569e-03  9.24528316e-02\n",
      " -7.13028153e-03 -5.09118848e-02 -1.25383623e-02  3.78009342e-02\n",
      " -9.14406180e-02  2.79138004e-03  8.85461047e-02 -4.49561402e-02\n",
      " -5.31745236e-03 -4.67836820e-02 -7.23720640e-02 -2.53253151e-02\n",
      "  7.81800821e-02  8.76229722e-04  2.97068362e-03  6.50811940e-02\n",
      " -2.53386013e-02  4.03856896e-02 -4.27052863e-02  5.85143119e-02\n",
      " -1.13920914e-02  2.23690830e-02 -6.20608814e-02 -5.62109202e-02\n",
      "  1.93309002e-02 -8.06603301e-03 -1.43439230e-02 -4.79530543e-02\n",
      " -5.74387722e-02  1.39466142e-02  6.89928979e-02 -2.73771901e-02\n",
      "  1.02233114e-02 -4.88367453e-02 -3.18277515e-02  6.15159236e-03\n",
      "  5.53811491e-02  1.18792526e-01  2.79352516e-02 -3.58289331e-02\n",
      "  6.67685717e-02 -2.61917058e-02 -5.48359640e-02 -2.41903169e-03\n",
      "  5.28179444e-02 -9.48879030e-03  1.26872197e-01 -2.06693970e-02\n",
      " -8.98289979e-02 -7.98647292e-03 -9.30376630e-03  4.24564164e-03\n",
      " -3.72129194e-02  9.69351605e-02 -2.97396146e-02 -1.25339534e-02\n",
      "  5.51020093e-02  5.96129410e-02 -4.90164571e-02 -2.71599106e-02\n",
      "  3.41912508e-02 -3.06354389e-02 -9.38249752e-03  1.22948969e-02\n",
      " -7.43849669e-03 -1.02433585e-01  5.06000295e-02 -2.26445738e-02\n",
      "  6.73964294e-03 -1.88290309e-02  4.31936719e-02 -1.82645188e-08\n",
      " -4.60406803e-02  2.46969294e-02 -6.20786138e-02 -8.98316409e-03\n",
      "  4.22681347e-02 -5.46964211e-03 -4.14383858e-02  2.54661627e-02\n",
      "  1.31141264e-02  3.16578187e-02  5.56757785e-02  1.42275970e-02\n",
      " -1.03649748e-02  9.81195942e-02 -6.13885038e-02  6.10173717e-02\n",
      "  9.29333642e-03  7.90136978e-02 -3.38275358e-02  2.38476563e-02\n",
      "  3.21798623e-02 -2.91430913e-02 -6.56148791e-02 -1.42004760e-02\n",
      "  1.02662757e-01 -2.12192088e-02 -6.33278415e-02 -2.80831065e-02\n",
      "  8.49515013e-03 -7.36703500e-02  1.55479014e-02  2.53564212e-02\n",
      "  2.03757938e-02 -1.25167847e-01 -3.71653251e-02  2.82582808e-02\n",
      " -6.29359391e-03 -9.49872285e-03  4.25446816e-02 -3.32132205e-02\n",
      "  2.33318433e-02  2.10316051e-02  5.14743365e-02 -2.30306331e-02\n",
      " -1.79145131e-02 -8.70079845e-02 -3.33489515e-02 -3.28541128e-03\n",
      " -3.86269465e-02 -3.36989723e-02 -2.88533941e-02  1.46411196e-03\n",
      "  1.14315815e-01 -1.72386710e-02  5.45899197e-02 -9.60597396e-02\n",
      "  1.58994626e-02  4.22969796e-02 -1.13363393e-01  2.01256499e-02\n",
      "  9.12697539e-02  2.07919441e-02  4.11036536e-02 -9.20403749e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo preentrenado\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Puedes cambiar el modelo si lo deseas\n",
    "\n",
    "# Leer el archivo de texto\n",
    "with open('../data/cooked/texto_extraido.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Generar embeddings para cada línea del archivo\n",
    "embeddings = model.encode(lines, convert_to_numpy=True)\n",
    "\n",
    "# Ver el primer embedding para verificar\n",
    "print(\"Embedding de la primera línea:\", embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de vectores en el índice: 1827\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Dimensión de los embeddings (depende del modelo que uses, por ejemplo, 384 para 'all-MiniLM-L6-v2')\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "# Crear el índice FAISS (usamos IndexFlatL2 para distancia euclidiana)\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Agregar los embeddings al índice\n",
    "index.add(embeddings)\n",
    "\n",
    "# Verifica cuántos vectores se han añadido\n",
    "print(\"Número de vectores en el índice:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = [\"cuando se gana una partida\"]\n",
    "consulta_embedding = model.encode(consulta, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices de los vectores más cercanos: [[483 760 937 655 370]]\n",
      "Distancias de los vectores más cercanos: [[0.5913608  0.5915155  0.595267   0.61422104 0.6148168 ]]\n"
     ]
    }
   ],
   "source": [
    "# Buscar los 5 vectores más cercanos\n",
    "k = 5\n",
    "distances, indices = index.search(consulta_embedding, k)\n",
    "\n",
    "print(\"Índices de los vectores más cercanos:\", indices)\n",
    "print(\"Distancias de los vectores más cercanos:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el índice\n",
    "faiss.write_index(index, \"mi_indice.index\")\n",
    "\n",
    "# Cargar el índice desde el archivo\n",
    "index_cargado = faiss.read_index(\"mi_indice.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han agregado 1827 embeddings al índice.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Número de dimensiones de los embeddings (debería coincidir con la salida de tu modelo)\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "# Crear el índice FAISS (puedes elegir el tipo de índice dependiendo de tus necesidades)\n",
    "index = faiss.IndexFlatL2(dim)  # IndexFlatL2 es para distancias L2\n",
    "\n",
    "# Agregar los embeddings al índice\n",
    "index.add(embeddings)  # embeddings es un array numpy de tamaño (número de textos, dimensiones del embedding)\n",
    "print(f\"Se han agregado {embeddings.shape[0]} embeddings al índice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento recuperado: Sin embargo, la partida es tablas si la posición es tal que el oponente no puede dar \n",
      "\n",
      "Distancia: 0.4856340289115906\n",
      "Documento recuperado: la consideración de una oferta de tablas. \n",
      "\n",
      "Distancia: 0.5014404058456421\n",
      "Documento recuperado: La partida es tablas cuando se alcanza una posición en la que ningún jugador puede \n",
      "\n",
      "Distancia: 0.5126779079437256\n",
      "Documento recuperado: Si la posición es tal que ninguno de los jugadores puede dar mate, la partida es tablas. (Ver \n",
      "\n",
      "Distancia: 0.5383651852607727\n",
      "Documento recuperado: embargo, decretará tablas si la posición es tal que el oponente no puede dar jaque \n",
      "\n",
      "Distancia: 0.5409550070762634\n"
     ]
    }
   ],
   "source": [
    "# Función para realizar la búsqueda en FAISS\n",
    "def buscar_similaridad(query, k=5):\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Realizar la búsqueda en FAISS\n",
    "    D, I = index.search(query_embedding, k)  # D es la distancia, I los índices\n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    for i in range(k):\n",
    "        print(f\"Documento recuperado: {lines[I[0][i]]}\\nDistancia: {D[0][i]}\")\n",
    "\n",
    "# Prueba de búsqueda con una consulta\n",
    "query = \"¿Qué es tablas?\"\n",
    "buscar_similaridad(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Inicializa la API de OpenAI\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minit_openai\u001b[39m():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Cargar las variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave de API de OpenAI desde las variables de entorno\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Cargar el modelo preentrenado para embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Cargar el índice FAISS previamente guardado\n",
    "index = faiss.read_index(\"mi_indice.index\")\n",
    "\n",
    "# Leer los documentos originales desde el archivo\n",
    "with open('../data/cooked/texto_extraido.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Función para buscar documentos relevantes en FAISS\n",
    "def buscar_documentos(query, k=5):\n",
    "    # Obtener el embedding para la consulta utilizando el modelo\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # Buscar en el índice de FAISS los documentos más cercanos\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Recuperar los textos de los documentos más cercanos\n",
    "    documents = [lines[i] for i in I[0]]\n",
    "    return documents\n",
    "\n",
    "# Función para crear un hilo de conversación en OpenAI\n",
    "def create_thread(openai_client):\n",
    "    \"\"\"\n",
    "    Crea un nuevo hilo de conversación en OpenAI.\n",
    "    \"\"\"\n",
    "    return openai_client.beta.threads.create()\n",
    "\n",
    "# Función para enviar un mensaje a OpenAI y obtener la respuesta\n",
    "def process_data(openai_client, assistant_id, thread_id, message):\n",
    "    \"\"\"\n",
    "    Envía un mensaje a un asistente de OpenAI y procesa su respuesta.\n",
    "    \"\"\"\n",
    "    openai_client.beta.threads.messages.create(\n",
    "        thread_id=thread_id,\n",
    "        role=\"user\",\n",
    "        content=message,\n",
    "    )\n",
    "\n",
    "    run = openai_client.beta.threads.runs.create(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id\n",
    "    )\n",
    "\n",
    "    run_status = openai_client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        run_status = openai_client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        if run_status.status == \"completed\":\n",
    "            break\n",
    "        elif run_status.status == \"failed\":\n",
    "            print(\"Error, no se encontró una respuesta del asistente.\")\n",
    "            return \"No se encontró una respuesta del asistente.\"\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "\n",
    "    response_messages = openai_client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    \n",
    "    assistant_response = None\n",
    "    for message in response_messages.data:\n",
    "            assistant_response = \"\\n\".join([block.text.value for block in message.content])\n",
    "            break\n",
    "\n",
    "    return assistant_response\n",
    "\n",
    "# Función para generar una respuesta utilizando los documentos recuperados\n",
    "def generar_respuesta(query):\n",
    "    # Obtener documentos relevantes de FAISS\n",
    "    documents = buscar_documentos(query)\n",
    "    \n",
    "    # Crear el contexto para el asistente, concatenando los documentos relevantes\n",
    "    contexto = \"\\n\".join(documents)\n",
    "\n",
    "    # Inicializa el cliente de OpenAI\n",
    "    openai_client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "    # Crear el hilo\n",
    "    thread = create_thread(openai_client)\n",
    "    thread_id = thread['id']\n",
    "\n",
    "    # Enviar la consulta al asistente\n",
    "    assistant_response = process_data(openai_client, assistant_id=\"assistant_id\", thread_id=thread_id, message=f\"Basado en los siguientes documentos, por favor, responde a la consulta: '{query}'\\n\\n{contexto}\")\n",
    "\n",
    "    return assistant_response\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Qué dice el documento sobre inteligencia artificial?\"\n",
    "respuesta = generar_respuesta(query)\n",
    "print(\"Respuesta generada:\", respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'Chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m consulta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¿Cuál es el propósito del sistema de búsqueda?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generar respuesta usando el sistema\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m \u001b[43mgenerar_respuesta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsulta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespuesta generada:\u001b[39m\u001b[38;5;124m\"\u001b[39m, respuesta)\n",
      "Cell \u001b[1;32mIn[23], line 45\u001b[0m, in \u001b[0;36mgenerar_respuesta\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     42\u001b[0m contexto \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(documents)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Realizar la consulta a GPT-4 usando la nueva interfaz de API\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChat\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     46\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Usamos GPT-4\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     48\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEres un asistente útil.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     49\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasado en los siguientes documentos, por favor, responde a la consulta: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontexto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     50\u001b[0m     ]\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Obtener la respuesta generada\u001b[39;00m\n\u001b[0;32m     54\u001b[0m respuesta \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Chat'"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta\n",
    "consulta = \"¿Cuál es el propósito del sistema de búsqueda?\"\n",
    "\n",
    "# Generar respuesta usando el sistema\n",
    "respuesta = generar_respuesta(consulta)\n",
    "\n",
    "print(\"Respuesta generada:\", respuesta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba_rag_tailor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
